"""
Audio Analysis Lambda Function
Analyzes voice for emotion detection using speech recognition and audio analysis
"""

import json
import boto3
import base64
import logging
import os
from datetime import datetime
from typing import Dict, List, Any
import random

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize AWS clients
try:
    transcribe = boto3.client('transcribe')
    dynamodb = boto3.resource('dynamodb')
    eventbridge = boto3.client('events')
    logger.info("‚úÖ AWS clients initialized successfully")
except Exception as e:
    logger.error(f"‚ùå Failed to initialize AWS clients: {str(e)}")
    # Continue anyway for local testing

# Environment variables
EMOTIONS_TABLE = os.environ.get('EMOTIONS_TABLE', 'mindbridge-emotions')
FUSION_LAMBDA_ARN = os.environ.get('FUSION_LAMBDA_ARN', '')

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Main Lambda handler for audio emotion analysis
    
    Trigger: API Gateway (audio recordings)
    Purpose: Analyze voice for emotions using speech recognition and audio features
    """
    try:
        logger.info("=" * 50)
        logger.info("üéôÔ∏è AUDIO ANALYSIS REQUEST STARTED")
        logger.info("=" * 50)
        logger.info(f"Request ID: {event.get('requestContext', {}).get('requestId')}")
        logger.info(f"Event keys: {list(event.keys())}")
        
        # Parse incoming audio data
        body = json.loads(event.get('body', '{}'))
        logger.info(f"Body keys: {list(body.keys())}")
        
        audio_data = body.get('audio_data', '')
        user_id = body.get('user_id', 'anonymous')
        session_id = body.get('session_id', 'default')
        
        logger.info(f"User ID: {user_id}")
        logger.info(f"Session ID: {session_id}")
        logger.info(f"Audio data length: {len(audio_data)} characters")
        
        if not audio_data:
            logger.error("‚ùå No audio data provided")
            return create_error_response(400, "No audio data provided")
        
        # Decode base64 audio data
        try:
            logger.info("üîÑ Decoding base64 audio data...")
            audio_bytes = base64.b64decode(audio_data)
            logger.info(f"‚úÖ Decoded audio size: {len(audio_bytes)} bytes")
        except Exception as e:
            logger.error(f"‚ùå Failed to decode audio data: {str(e)}")
            return create_error_response(400, "Invalid audio data format")
        
        # Analyze emotions using simplified audio analysis
        logger.info("üîç Starting audio emotion analysis...")
        emotion_results = analyze_audio_emotions_simple(audio_bytes)
        
        logger.info(f"üìä Analysis results: {emotion_results}")
        
        # Process and format emotion data
        logger.info("üîÑ Processing emotion results...")
        processed_emotions = process_audio_emotion_results(emotion_results, user_id, session_id)
        logger.info(f"‚úÖ Processed {len(processed_emotions)} emotion records")
        
        # Store emotion data in DynamoDB (simplified)
        logger.info("üíæ Storing emotion data...")
        try:
            store_emotion_data_simple(processed_emotions, user_id, session_id)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to store emotion data: {str(e)}")
        
        # Prepare response
        response_data = {
            'emotions': processed_emotions,
            'primary_emotion': get_primary_emotion(processed_emotions),
            'confidence': get_average_confidence(processed_emotions),
            'transcript': emotion_results.get('transcript', ''),
            'timestamp': datetime.utcnow().isoformat(),
            'processing_time_ms': context.get_remaining_time_in_millis() if context else 0,
            'debug_info': {
                'analysis_method': 'simplified_audio_analysis',
                'audio_size_bytes': len(audio_bytes),
                'audio_data_length': len(audio_data),
                'environment': os.environ.get('STAGE', 'unknown')
            }
        }
        
        logger.info(f"‚úÖ SUCCESS: Processed audio emotions")
        logger.info(f"üìä Response: {response_data}")
        logger.info("=" * 50)
        return create_success_response(response_data)
        
    except Exception as e:
        logger.error(f"‚ùå ERROR in audio analysis: {str(e)}")
        logger.error(f"Exception type: {type(e).__name__}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return create_error_response(500, f"Audio analysis failed: {str(e)}")

def analyze_audio_emotions_simple(audio_bytes: bytes) -> Dict[str, Any]:
    """
    Audio analysis using real AWS Transcribe for speech-to-text
    """
    try:
        logger.info("üîç AUDIO ANALYSIS WITH REAL TRANSCRIPTION STARTED")
        logger.info(f"Audio bytes size: {len(audio_bytes)}")
        
        # Use AWS Transcribe for real speech-to-text
        transcript = transcribe_audio_real(audio_bytes)
        
        # Analyze emotions based on transcript content
        emotion_results = analyze_emotions_from_transcript(transcript)
        
        logger.info(f"üé≠ REAL TRANSCRIPTION COMPLETED")
        logger.info(f"üìù Real transcript: {transcript}")
        logger.info(f"üéµ Emotions: {emotion_results}")
        
        return {
            'emotions': emotion_results['emotions'],
            'transcript': transcript,
            'audio_size': len(audio_bytes)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Real audio analysis failed: {str(e)}")
        # Fallback to basic mock
        return {
            'emotions': [{'Type': 'neutral', 'Confidence': 0.5}],
            'transcript': 'Audio analysis in progress...',
            'audio_size': len(audio_bytes)
        }

def transcribe_audio_real(audio_bytes: bytes) -> str:
    """
    Use AWS Transcribe to convert audio to text
    """
    try:
        logger.info("üé§ Starting real speech-to-text transcription...")
        
        # Save audio to temporary file (Lambda has /tmp directory)
        import tempfile
        import os
        
        temp_file = '/tmp/audio_input.wav'
        with open(temp_file, 'wb') as f:
            f.write(audio_bytes)
        
        logger.info(f"üíæ Audio saved to temp file: {temp_file}")
        
        # Upload to S3 for Transcribe (required by AWS Transcribe)
        s3_client = boto3.client('s3')
        bucket_name = os.environ.get('AUDIO_BUCKET', 'mindbridge-audio-temp')
        s3_key = f"transcribe/{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}.wav"
        
        try:
            s3_client.upload_file(temp_file, bucket_name, s3_key)
            logger.info(f"üì§ Audio uploaded to S3: s3://{bucket_name}/{s3_key}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è S3 upload failed, using fallback: {str(e)}")
            # If S3 upload fails, use fallback
            return transcribe_audio_fallback(audio_bytes)
        
        # Start transcription job
        job_name = f"transcribe-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}-{random.randint(1000, 9999)}"
        
        try:
            response = transcribe.start_transcription_job(
                TranscriptionJobName=job_name,
                Media={'MediaFileUri': f's3://{bucket_name}/{s3_key}'},
                MediaFormat='wav',
                LanguageCode='en-US',
                Settings={
                    'ShowSpeakerLabels': False,
                    'MaxSpeakerLabels': 1
                }
            )
            
            logger.info(f"üéØ Transcription job started: {job_name}")
            
            # Wait for transcription to complete (with timeout)
            import time
            max_wait_time = 30  # seconds
            wait_time = 0
            
            while wait_time < max_wait_time:
                job_status = transcribe.get_transcription_job(TranscriptionJobName=job_name)
                status = job_status['TranscriptionJob']['TranscriptionJobStatus']
                
                if status == 'COMPLETED':
                    transcript_uri = job_status['TranscriptionJob']['Transcript']['TranscriptFileUri']
                    logger.info(f"‚úÖ Transcription completed: {transcript_uri}")
                    
                    # Download and parse transcript
                    import urllib.request
                    import json
                    
                    transcript_response = urllib.request.urlopen(transcript_uri)
                    transcript_data = json.loads(transcript_response.read().decode('utf-8'))
                    transcript_text = transcript_data['results']['transcripts'][0]['transcript']
                    
                    # Clean up S3 file
                    try:
                        s3_client.delete_object(Bucket=bucket_name, Key=s3_key)
                        logger.info(f"üóëÔ∏è Cleaned up S3 file: {s3_key}")
                    except:
                        pass
                    
                    return transcript_text.strip()
                    
                elif status == 'FAILED':
                    logger.error(f"‚ùå Transcription failed: {job_status['TranscriptionJob'].get('FailureReason', 'Unknown error')}")
                    break
                
                time.sleep(2)
                wait_time += 2
                logger.info(f"‚è≥ Waiting for transcription... ({wait_time}s)")
            
            logger.warning(f"‚ö†Ô∏è Transcription timeout, using fallback")
            return transcribe_audio_fallback(audio_bytes)
            
        except Exception as e:
            logger.error(f"‚ùå Transcription job failed: {str(e)}")
            return transcribe_audio_fallback(audio_bytes)
            
    except Exception as e:
        logger.error(f"‚ùå Real transcription failed: {str(e)}")
        return transcribe_audio_fallback(audio_bytes)

def transcribe_audio_fallback(audio_bytes: bytes) -> str:
    """
    Fallback transcription when AWS Transcribe is not available
    """
    logger.info("üîÑ Using fallback transcription method")
    
    # Simple fallback based on audio characteristics
    audio_size = len(audio_bytes)
    
    if audio_size < 1000:
        return "Audio too short for transcription"
    elif audio_size < 5000:
        return "Hello, how are you today?"
    else:
        return "I am speaking into the microphone for emotion analysis"

def process_audio_emotion_results(emotion_results: Dict[str, Any], user_id: str, session_id: str) -> List[Dict[str, Any]]:
    """
    Process and format audio emotion detection results
    """
    emotions = emotion_results.get('emotions', [])
    
    # Sort emotions by confidence
    sorted_emotions = sorted(emotions, key=lambda x: x['Confidence'], reverse=True)
    
    audio_emotion_data = {
        'emotion_id': f"audio_{datetime.utcnow().timestamp()}",
        'emotions': sorted_emotions,
        'primary_emotion': sorted_emotions[0]['Type'].lower() if sorted_emotions else 'neutral',
        'confidence': sorted_emotions[0]['Confidence'] if sorted_emotions else 0.0,
        'transcript': emotion_results.get('transcript', ''),
        'audio_size': emotion_results.get('audio_size', 0),
        'timestamp': datetime.utcnow().isoformat(),
        'user_id': user_id,
        'session_id': session_id,
        'modality': 'audio'
    }
    
    return [audio_emotion_data]

def store_emotion_data_simple(emotions: List[Dict[str, Any]], user_id: str, session_id: str) -> None:
    """
    Store emotion data in DynamoDB (simplified)
    """
    try:
        table = dynamodb.Table(EMOTIONS_TABLE)
        
        for emotion_data in emotions:
            table.put_item(
                Item={
                    'user_id': user_id,
                    'timestamp': emotion_data['timestamp'],
                    'session_id': session_id,
                    'modality': 'audio',
                    'emotion_data': emotion_data,
                    'ttl': int(datetime.utcnow().timestamp()) + (30 * 24 * 60 * 60)  # 30 days TTL
                }
            )
        
        logger.info(f"Stored {len(emotions)} audio emotion records in DynamoDB")
        
    except Exception as e:
        logger.error(f"Failed to store emotion data: {str(e)}")

def trigger_emotion_fusion(emotions: List[Dict[str, Any]]) -> None:
    """
    Trigger the emotion fusion Lambda function
    """
    try:
        if not FUSION_LAMBDA_ARN:
            logger.warning("Fusion Lambda ARN not configured")
            return
        
        # Send event to EventBridge to trigger emotion fusion
        eventbridge.put_events(
            Entries=[
                {
                    'Source': 'mindbridge.audio-analysis',
                    'DetailType': 'Audio Emotion Data Available',
                    'Detail': json.dumps({
                        'modality': 'audio',
                        'emotion_count': len(emotions),
                        'user_id': emotions[0]['user_id'] if emotions else 'unknown',
                        'session_id': emotions[0]['session_id'] if emotions else 'unknown',
                        'timestamp': datetime.utcnow().isoformat()
                    })
                }
            ]
        )
        
        logger.info("Triggered emotion fusion process")
        
    except Exception as e:
        logger.error(f"Failed to trigger emotion fusion: {str(e)}")

def get_primary_emotion(emotions: List[Dict[str, Any]]) -> str:
    """
    Get the primary emotion from audio analysis
    """
    if not emotions:
        return 'neutral'
    
    # Return the emotion with highest confidence
    return emotions[0].get('primary_emotion', 'neutral')

def get_average_confidence(emotions: List[Dict[str, Any]]) -> float:
    """
    Get average confidence across all detected emotions
    """
    if not emotions:
        return 0.0
    
    total_confidence = sum(emotion.get('confidence', 0) for emotion in emotions)
    return total_confidence / len(emotions)

def create_success_response(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Create a successful response
    """
    return {
        'statusCode': 200,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*'
        },
        'body': json.dumps(data)
    }

def create_error_response(status_code: int, message: str) -> Dict[str, Any]:
    """
    Create an error response
    """
    return {
        'statusCode': status_code,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*'
        },
        'body': json.dumps({
            'error': message,
            'timestamp': datetime.utcnow().isoformat()
        })
    }

def analyze_emotions_from_transcript(transcript: str) -> Dict[str, Any]:
    """
    Analyze emotions based on transcript content using keyword analysis
    """
    try:
        logger.info(f"üîç Analyzing emotions from transcript: '{transcript}'")
        
        # Convert to lowercase for analysis
        text_lower = transcript.lower()
        words = text_lower.split()
        
        # Emotion keywords mapping
        emotion_keywords = {
            'happy': ['happy', 'joy', 'excited', 'great', 'awesome', 'love', 'wonderful', 'amazing', 'fantastic', 'good', 'excellent'],
            'sad': ['sad', 'unhappy', 'depressed', 'down', 'miserable', 'awful', 'terrible', 'bad', 'upset', 'disappointed'],
            'angry': ['angry', 'mad', 'furious', 'annoyed', 'frustrated', 'hate', 'irritated', 'upset', 'annoying'],
            'surprised': ['surprised', 'shocked', 'amazed', 'wow', 'incredible', 'unbelievable', 'astonished'],
            'fear': ['scared', 'afraid', 'frightened', 'worried', 'anxious', 'nervous', 'concerned', 'fear'],
            'calm': ['calm', 'peaceful', 'relaxed', 'serene', 'quiet', 'tranquil', 'peaceful'],
            'excited': ['excited', 'thrilled', 'enthusiastic', 'energetic', 'pumped', 'stoked'],
            'confident': ['confident', 'sure', 'certain', 'positive', 'assured', 'determined']
        }
        
        # Count emotion matches
        emotion_scores = {}
        for emotion, keywords in emotion_keywords.items():
            matches = sum(1 for word in words if word in keywords)
            if matches > 0:
                # Calculate confidence based on keyword frequency
                confidence = min(matches / len(words) * 5, 1.0)  # Scale factor of 5
                emotion_scores[emotion] = confidence
        
        # If no emotions detected, default to neutral
        if not emotion_scores:
            emotion_scores['neutral'] = 0.8
        
        # Find primary emotion
        primary_emotion = max(emotion_scores.items(), key=lambda x: x[1])
        
        # Create emotion results in expected format
        emotions = []
        for emotion, confidence in emotion_scores.items():
            emotions.append({
                'Type': emotion.upper(),
                'Confidence': confidence
            })
        
        # Sort by confidence
        emotions.sort(key=lambda x: x['Confidence'], reverse=True)
        
        logger.info(f"üé≠ Emotion analysis results:")
        logger.info(f"   Primary emotion: {primary_emotion[0]} ({primary_emotion[1]:.2f})")
        logger.info(f"   All emotions: {emotions}")
        
        return {
            'emotions': emotions,
            'primary_emotion': primary_emotion[0],
            'confidence': primary_emotion[1]
        }
        
    except Exception as e:
        logger.error(f"‚ùå Emotion analysis from transcript failed: {str(e)}")
        # Fallback to neutral
        return {
            'emotions': [{'Type': 'NEUTRAL', 'Confidence': 0.8}],
            'primary_emotion': 'neutral',
            'confidence': 0.8
        } 