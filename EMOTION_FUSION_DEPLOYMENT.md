# ðŸ§  Advanced Multi-Modal Emotion Fusion Lambda Function

## ðŸ“– Overview

This is a production-ready Lambda function that combines emotion analysis from video (facial expressions), audio (voice patterns), and text (sentiment) into unified emotional insights with AI-powered recommendations and risk assessment.

## ðŸš€ Key Features

### Advanced Fusion Logic
- **Multi-Modal Integration**: Combines video, audio, and text emotion data
- **Weighted Fusion Algorithm**: Dynamic weights based on data quality and modality reliability
- **Temporal Analysis**: Tracks emotion trends and patterns over time
- **AI Enhancement**: Uses Amazon Bedrock (Claude) for contextual insights
- **Risk Assessment**: Comprehensive risk scoring with automatic alerts

### Smart Recommendations
- **Immediate Actions**: Breathing exercises, grounding techniques
- **Short-term Strategies**: Coping mechanisms, trigger identification
- **Long-term Solutions**: Therapy recommendations, habit building
- **Priority-based**: Critical, high, medium, low priority levels

### Production Features
- **Comprehensive Logging**: Detailed logging with emojis for easy debugging
- **Error Handling**: Graceful fallbacks and error recovery
- **Multiple Event Sources**: EventBridge, API Gateway, direct invocation
- **DynamoDB Integration**: Stores fusion results for historical analysis
- **Real-time Alerts**: High-risk emotional states trigger immediate notifications

## ðŸ“‹ Prerequisites

### AWS Services Required
- **AWS Lambda** (main compute)
- **Amazon DynamoDB** (emotion data storage)
- **Amazon EventBridge** (event routing)
- **Amazon Bedrock** (AI enhancement - optional)
- **IAM Roles** (appropriate permissions)

### Environment Variables
```bash
EMOTIONS_TABLE=mindbridge-emotions
BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
```

## ðŸ”§ Deployment Instructions

### Step 1: Replace Basic Lambda Function

1. **Open AWS Console** â†’ Navigate to Lambda
2. **Find your emotion fusion function** (currently shows basic template)
3. **Replace the code** with contents of `aws_fusion_handler.py`
4. **Configure environment variables** as listed above

### Step 2: Set IAM Permissions

Your Lambda execution role needs these permissions:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "dynamodb:Query",
                "dynamodb:PutItem",
                "dynamodb:GetItem"
            ],
            "Resource": "arn:aws:dynamodb:*:*:table/mindbridge-emotions"
        },
        {
            "Effect": "Allow",
            "Action": [
                "events:PutEvents"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel"
            ],
            "Resource": "arn:aws:bedrock:*::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0"
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
}
```

### Step 3: Configure Function Settings

- **Runtime**: Python 3.9 or later
- **Memory**: 512 MB (minimum for numpy operations)
- **Timeout**: 60 seconds (allows for AI processing)
- **Handler**: `lambda_function.lambda_handler`

### Step 4: Add Dependencies

The function requires these Python packages (add via Lambda Layers or include in deployment package):

```txt
boto3>=1.34.0
numpy>=1.24.0
```

## ðŸ”„ How It Works

### Input Processing
1. **Event Detection**: Handles EventBridge events from video/audio/text analysis
2. **Data Collection**: Queries recent emotion data from DynamoDB (5-minute window)
3. **Modality Separation**: Groups emotions by video, audio, text sources

### Fusion Algorithm
1. **Modality Analysis**: Analyzes each modality for primary emotion, confidence, stability
2. **Weight Calculation**: Dynamic weights based on data quality and reliability
3. **Weighted Fusion**: Combines emotions using calculated weights
4. **Temporal Analysis**: Identifies trends, volatility, and patterns
5. **Risk Assessment**: Calculates risk scores and levels
6. **AI Enhancement**: Optional Claude analysis for contextual insights

### Output Generation
1. **Unified Emotion**: Single primary emotion with confidence and intensity
2. **Smart Recommendations**: Immediate, short-term, and long-term actions
3. **Risk Alerts**: Automatic notifications for high-risk states
4. **Historical Storage**: Results stored in DynamoDB

## ðŸ“Š Example Fusion Logic

### Multi-Modal Data Input
```json
{
  "video": {
    "primary_emotion": "sad",
    "confidence": 0.85,
    "stability": 0.9
  },
  "audio": {
    "primary_emotion": "stressed",
    "confidence": 0.75,
    "stability": 0.6
  },
  "text": {
    "primary_emotion": "neutral",
    "confidence": 0.6,
    "stability": 0.8
  }
}
```

### Weighted Fusion Calculation
```python
# Base weights
audio_weight = 0.4 * (0.75 * 0.6)^0.5 = 0.268
video_weight = 0.35 * (0.85 * 0.9)^0.5 = 0.306  
text_weight = 0.25 * (0.6 * 0.8)^0.5 = 0.173

# Normalize weights
total = 0.747
final_weights = {
  "video": 0.41,    # Highest quality data
  "audio": 0.36,    # Medium quality  
  "text": 0.23      # Lower quality
}
```

### Unified Output
```json
{
  "primary_emotion": "sad",
  "confidence": 0.79,
  "intensity": 8,
  "risk_assessment": {
    "level": "medium",
    "score": 2.5
  },
  "recommendations": {
    "immediate": ["Take slow, deep breaths", "Reach out to a friend"],
    "priority": "medium"
  }
}
```

## ðŸš¨ Risk Assessment Logic

### Risk Factors
- **Emotion Type**: angry, fear, stressed = high risk
- **Intensity Level**: 8+ = critical, 6+ = high
- **Confidence Level**: High confidence in negative emotions increases risk
- **Cross-Modal Agreement**: All modalities showing same negative emotion
- **Temporal Trends**: Declining emotional state over time

### Risk Levels
- **Critical (5.0+)**: Immediate intervention required
- **High (4.0+)**: Close monitoring and support needed  
- **Medium (2.5+)**: Attention and coping strategies recommended
- **Low (<2.5)**: Normal emotional state

## ðŸ¤– AI Enhancement

When Amazon Bedrock is available and sufficient data exists (3+ emotion records), the function uses Claude to:

- **Validate Fusion Results**: Cross-check algorithmic fusion with AI insights
- **Contextual Analysis**: Identify patterns the algorithm might miss
- **Confidence Adjustment**: Fine-tune confidence based on data quality
- **Enhanced Recommendations**: AI-generated personalized suggestions

## ðŸ“ˆ Monitoring & Debugging

### CloudWatch Logs
The function provides extensive logging:
- ðŸš€ Process start/completion
- ðŸ“Š Data collection and analysis
- âš–ï¸ Fusion weight calculations
- ðŸš¨ Risk assessments and alerts
- ðŸ’¡ Recommendation generation
- âŒ Error handling and fallbacks

### Key Metrics to Monitor
- **Invocation Count**: How often fusion is triggered
- **Error Rate**: Failed fusion attempts
- **Duration**: Processing time (should be <30s typically)
- **Risk Alerts**: High/critical risk detections

## ðŸ”§ Troubleshooting

### Common Issues

1. **No emotion data found**
   - Check if other Lambda functions are storing data correctly
   - Verify DynamoDB table permissions and structure

2. **AI enhancement failing**
   - Ensure Bedrock permissions are set
   - Check if Claude model is available in your region

3. **High memory usage**
   - Increase Lambda memory allocation
   - Check if numpy operations are optimized

4. **Timeout errors**
   - Increase Lambda timeout setting
   - Optimize data queries and processing

## ðŸ”„ Testing

### Local Testing
```python
# Test event for local development
test_event = {
    "user_id": "test-user",
    "session_id": "test-session",
    "detail": {
        "modality": "video",
        "user_id": "test-user",
        "session_id": "test-session"
    }
}

# Run function
result = lambda_handler(test_event, None)
print(json.dumps(result, indent=2))
```

### Integration Testing
1. Trigger video/audio/text analysis functions
2. Wait for fusion to be triggered via EventBridge
3. Check CloudWatch logs for fusion processing
4. Verify results in DynamoDB

## ðŸ“š Further Reading

- [AWS Lambda Developer Guide](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)
- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [DynamoDB Best Practices](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html)
- [EventBridge User Guide](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html)

---

## ðŸŽ¯ Quick Deployment Checklist

- [ ] Copy `aws_fusion_handler.py` to Lambda function
- [ ] Set environment variables (EMOTIONS_TABLE, BEDROCK_MODEL_ID)
- [ ] Configure IAM permissions for DynamoDB, EventBridge, Bedrock
- [ ] Set memory to 512MB+ and timeout to 60s
- [ ] Test with sample event
- [ ] Monitor CloudWatch logs for successful processing
- [ ] Verify data storage in DynamoDB
- [ ] Test high-risk scenarios for alert functionality

**Your advanced emotion fusion Lambda function is now ready for production! ðŸš€**
